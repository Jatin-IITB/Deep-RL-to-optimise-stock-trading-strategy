{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1a2ee9-b0d2-44de-a1a9-907f700c3d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import ta\n",
    "\n",
    "def calculate_indicators(df):\n",
    "    # Exponential Moving Average (EMA)\n",
    "    df['EMA_12'] = ta.trend.EMAIndicator(df['Close'], window=12).ema_indicator()\n",
    "    df['EMA_26'] = ta.trend.EMAIndicator(df['Close'], window=26).ema_indicator()\n",
    "\n",
    "    # Moving Average Convergence Divergence (MACD)\n",
    "    macd = ta.trend.MACD(df['Close'])\n",
    "    df['MACD'] = macd.macd()\n",
    "    df['MACD_signal'] = macd.macd_signal()\n",
    "\n",
    "    # Volume Weighted Average Price (VWAP)\n",
    "    df['VWAP'] = ta.volume.VolumeWeightedAveragePrice(df['High'], df['Low'], df['Close'], df['Volume']).volume_weighted_average_price()\n",
    "\n",
    "    # Relative Strength Index (RSI)\n",
    "    df['RSI'] = ta.momentum.RSIIndicator(df['Close']).rsi()\n",
    "\n",
    "    # Stochastic Oscillator\n",
    "    df['Stochastic'] = ta.momentum.StochasticOscillator(df['High'], df['Low'], df['Close']).stoch()\n",
    "\n",
    "    # Average True Range (ATR)\n",
    "    df['ATR'] = ta.volatility.AverageTrueRange(df['High'], df['Low'], df['Close']).average_true_range()\n",
    "\n",
    "    # Supertrend\n",
    "    df['Supertrend'] = calculate_supertrend(df)\n",
    "\n",
    "    # Calculate Bollinger Bands\n",
    "    bollinger = ta.volatility.BollingerBands(df['Close'])\n",
    "    df['BB_Upper'] = bollinger.bollinger_hband()\n",
    "    df['BB_Lower'] = bollinger.bollinger_lband()\n",
    "\n",
    "    # Calculate Fibonacci Levels\n",
    "    df = calculate_fibonacci_levels(df)\n",
    "\n",
    "    # Calculate Momentum manually\n",
    "    period = 10\n",
    "    df['Momentum'] = df['Close'].diff(periods=period)\n",
    "\n",
    "    # Drop rows with NaN values\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def calculate_supertrend(df, period=14, multiplier=3):\n",
    "    # Calculate ATR\n",
    "    df['ATR'] = ta.volatility.AverageTrueRange(df['High'], df['Low'], df['Close'], window=period).average_true_range()\n",
    "\n",
    "    # Initialize columns for Supertrend calculation\n",
    "    df['Supertrend'] = pd.Series(index=df.index)\n",
    "    df['Upper_Band'] = pd.Series(index=df.index)\n",
    "    df['Lower_Band'] = pd.Series(index=df.index)\n",
    "    \n",
    "    # Calculate initial Supertrend values\n",
    "    df['Upper_Band'] = df['Close'] + (df['ATR'] * multiplier)\n",
    "    df['Lower_Band'] = df['Close'] - (df['ATR'] * multiplier)\n",
    "    \n",
    "    # Set initial Supertrend value\n",
    "    df.loc[df.index[0], 'Supertrend'] = df.loc[df.index[0], 'Lower_Band']\n",
    "\n",
    "    # Iterate over rows to calculate Supertrend\n",
    "    for i in range(1, len(df)):\n",
    "        if df.loc[df.index[i-1], 'Close'] > df.loc[df.index[i-1], 'Supertrend']:\n",
    "            df.loc[df.index[i], 'Supertrend'] = max(df.loc[df.index[i], 'Lower_Band'], df.loc[df.index[i-1], 'Supertrend'])\n",
    "        else:\n",
    "            df.loc[df.index[i], 'Supertrend'] = df.loc[df.index[i], 'Upper_Band']\n",
    "    \n",
    "    return df['Supertrend']\n",
    "\n",
    "def calculate_fibonacci_levels(df):\n",
    "    # Assume we use the last 50 days to calculate Fibonacci levels\n",
    "    window = 50\n",
    "    df['Fibonacci_R1'] = df['Close'].rolling(window=window).apply(lambda x: max(x) - (0.236 * (max(x) - min(x))))\n",
    "    df['Fibonacci_R2'] = df['Close'].rolling(window=window).apply(lambda x: max(x) - (0.382 * (max(x) - min(x))))\n",
    "    df['Fibonacci_S1'] = df['Close'].rolling(window=window).apply(lambda x: min(x) + (0.236 * (max(x) - min(x))))\n",
    "    df['Fibonacci_S2'] = df['Close'].rolling(window=window).apply(lambda x: min(x) + (0.382 * (max(x) - min(x))))\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "symbol = 'TCS.BO'\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2024-07-31'\n",
    "data = yf.download(symbol, start=start_date, end=end_date)\n",
    "data = calculate_indicators(data)\n",
    "data.to_csv(f\"{symbol}_with_indicators.csv\")\n",
    "\n",
    "print(f\"Data saved to {symbol}_with_indicators.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39d18393-5652-4bc3-a1f3-84b0be2a1889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Episode 1/100, Total Reward: 33971.90554659999\n",
      "Episode 2/100, Total Reward: 29200.91495499997\n",
      "Episode 3/100, Total Reward: 31705.795959500043\n",
      "Episode 4/100, Total Reward: 33618.51288409996\n",
      "Episode 5/100, Total Reward: 80174.26941399983\n",
      "Episode 6/100, Total Reward: 101350.74116229994\n",
      "Episode 7/100, Total Reward: 38205.91336149995\n",
      "Episode 8/100, Total Reward: 50002.48266579999\n",
      "Episode 9/100, Total Reward: 40149.685786200105\n",
      "Episode 10/100, Total Reward: 52756.66696830004\n",
      "Episode 11/100, Total Reward: 45088.524713199964\n",
      "Episode 12/100, Total Reward: 87684.40048450003\n",
      "Episode 13/100, Total Reward: 101365.86669299997\n",
      "Episode 14/100, Total Reward: 39359.78935539999\n",
      "Episode 15/100, Total Reward: 15258.645888900057\n",
      "Episode 16/100, Total Reward: 86235.67294699993\n",
      "Episode 17/100, Total Reward: 37733.32113790002\n",
      "Episode 18/100, Total Reward: 124701.59880690007\n",
      "Episode 19/100, Total Reward: 93618.32988559986\n",
      "Episode 20/100, Total Reward: 21850.88310529997\n",
      "Episode 21/100, Total Reward: 30170.328306899995\n",
      "Episode 22/100, Total Reward: 132704.22003720008\n",
      "Episode 23/100, Total Reward: 74920.85850219999\n",
      "Episode 24/100, Total Reward: 37734.6345954001\n",
      "Episode 25/100, Total Reward: 20450.21709920001\n",
      "Episode 26/100, Total Reward: 93147.82243270011\n",
      "Episode 27/100, Total Reward: 68398.43457989997\n",
      "Episode 28/100, Total Reward: -12369.439540900028\n",
      "Episode 29/100, Total Reward: 12072.575064299997\n",
      "Episode 30/100, Total Reward: 46923.4985043\n",
      "Episode 31/100, Total Reward: 124674.77858180016\n",
      "Episode 32/100, Total Reward: 156154.85348009985\n",
      "Episode 33/100, Total Reward: 100944.13850299994\n",
      "Episode 34/100, Total Reward: 39873.097071399956\n",
      "Episode 35/100, Total Reward: 13158.33054339997\n",
      "Episode 36/100, Total Reward: 96398.44904390007\n",
      "Episode 37/100, Total Reward: 9296.815281399984\n",
      "Episode 38/100, Total Reward: 19772.481098200038\n",
      "Episode 39/100, Total Reward: 40636.519518000016\n",
      "Episode 40/100, Total Reward: 109898.42713759979\n",
      "Episode 41/100, Total Reward: 64837.47615289997\n",
      "Episode 42/100, Total Reward: 39357.89699339997\n",
      "Episode 43/100, Total Reward: 47653.8338372\n",
      "Episode 44/100, Total Reward: 45978.64471709996\n",
      "Episode 45/100, Total Reward: 13914.36179259995\n",
      "Episode 46/100, Total Reward: 18405.879288899956\n",
      "Episode 47/100, Total Reward: 42984.416779200124\n",
      "Episode 48/100, Total Reward: 115460.66438300005\n",
      "Episode 49/100, Total Reward: -1042.824319899978\n",
      "Episode 50/100, Total Reward: 93503.62481079994\n",
      "Episode 51/100, Total Reward: 17970.179392900012\n",
      "Episode 52/100, Total Reward: 18304.98927529994\n",
      "Episode 53/100, Total Reward: -22755.91901479999\n",
      "Episode 54/100, Total Reward: 2921.9191487999506\n",
      "Episode 55/100, Total Reward: 67935.71370280003\n",
      "Episode 56/100, Total Reward: 93601.33057919997\n",
      "Episode 57/100, Total Reward: 80462.36992300017\n",
      "Episode 58/100, Total Reward: 4625.52111289999\n",
      "Episode 59/100, Total Reward: 24760.810328\n",
      "Episode 60/100, Total Reward: -2159.4777416000497\n",
      "Episode 61/100, Total Reward: 219102.0324924999\n",
      "Episode 62/100, Total Reward: 112605.6272825999\n",
      "Episode 63/100, Total Reward: 12879.915157500032\n",
      "Episode 64/100, Total Reward: 46701.821239600045\n",
      "Episode 65/100, Total Reward: 20835.49798709999\n",
      "Episode 66/100, Total Reward: 51093.922811199955\n",
      "Episode 67/100, Total Reward: 183587.18619309997\n",
      "Episode 68/100, Total Reward: 102787.67434080012\n",
      "Episode 69/100, Total Reward: 117697.3488209\n",
      "Episode 70/100, Total Reward: 113421.48272760012\n",
      "Episode 71/100, Total Reward: 23573.25329939997\n",
      "Episode 72/100, Total Reward: 99459.87703469995\n",
      "Episode 73/100, Total Reward: 74387.83243389998\n",
      "Episode 74/100, Total Reward: 6239.715348600001\n",
      "Episode 75/100, Total Reward: 14072.051856199972\n",
      "Episode 76/100, Total Reward: 134550.4008454\n",
      "Episode 77/100, Total Reward: 187370.4940244002\n",
      "Episode 78/100, Total Reward: 58069.64802799997\n",
      "Episode 79/100, Total Reward: 53541.790690199996\n",
      "Episode 80/100, Total Reward: 6070.9878795999975\n",
      "Episode 81/100, Total Reward: 21895.858116199994\n",
      "Episode 82/100, Total Reward: 45041.31054069997\n",
      "Episode 83/100, Total Reward: 146661.75243790002\n",
      "Episode 84/100, Total Reward: 97733.4006745\n",
      "Episode 85/100, Total Reward: 115471.64364260013\n",
      "Episode 86/100, Total Reward: 189899.4618963996\n",
      "Episode 87/100, Total Reward: 118674.47433189997\n",
      "Episode 88/100, Total Reward: 34570.07919069997\n",
      "Episode 89/100, Total Reward: 27396.680986499974\n",
      "Episode 90/100, Total Reward: 102024.09196290001\n",
      "Episode 91/100, Total Reward: 158219.4529614001\n",
      "Episode 92/100, Total Reward: 150540.97626719993\n",
      "Episode 93/100, Total Reward: 191210.9402992999\n",
      "Episode 94/100, Total Reward: 186547.60764299985\n",
      "Episode 95/100, Total Reward: 127932.5439823001\n",
      "Episode 96/100, Total Reward: 92626.06816120003\n",
      "Episode 97/100, Total Reward: 139098.88548249984\n",
      "Episode 98/100, Total Reward: 181341.16107859998\n",
      "Episode 99/100, Total Reward: 68631.93234860002\n",
      "Episode 100/100, Total Reward: 170960.82030969975\n",
      "Total Reward of RL Agent: 45252.08614400001\n",
      "ARIMA Model - Annualized Returns: 4.0811049463573, Sharpe Ratio: 0.9991633346111133, Max Drawdown: 3.2832325957219837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "89/89 - 7s - 73ms/step - loss: 2357871.7500\n",
      "Epoch 2/100\n",
      "89/89 - 0s - 3ms/step - loss: 2337718.7500\n",
      "Epoch 3/100\n",
      "89/89 - 0s - 2ms/step - loss: 2309453.0000\n",
      "Epoch 4/100\n",
      "89/89 - 0s - 2ms/step - loss: 2292871.7500\n",
      "Epoch 5/100\n",
      "89/89 - 0s - 2ms/step - loss: 2280138.0000\n",
      "Epoch 6/100\n",
      "89/89 - 0s - 2ms/step - loss: 2268738.0000\n",
      "Epoch 7/100\n",
      "89/89 - 0s - 2ms/step - loss: 2258080.2500\n",
      "Epoch 8/100\n",
      "89/89 - 0s - 2ms/step - loss: 2247804.5000\n",
      "Epoch 9/100\n",
      "89/89 - 0s - 2ms/step - loss: 2237830.7500\n",
      "Epoch 10/100\n",
      "89/89 - 0s - 3ms/step - loss: 2228085.5000\n",
      "Epoch 11/100\n",
      "89/89 - 0s - 4ms/step - loss: 2218517.0000\n",
      "Epoch 12/100\n",
      "89/89 - 0s - 4ms/step - loss: 2209120.0000\n",
      "Epoch 13/100\n",
      "89/89 - 0s - 3ms/step - loss: 2199787.2500\n",
      "Epoch 14/100\n",
      "89/89 - 0s - 2ms/step - loss: 2190594.2500\n",
      "Epoch 15/100\n",
      "89/89 - 0s - 3ms/step - loss: 2181439.7500\n",
      "Epoch 16/100\n",
      "89/89 - 0s - 3ms/step - loss: 2172366.2500\n",
      "Epoch 17/100\n",
      "89/89 - 0s - 3ms/step - loss: 2163343.5000\n",
      "Epoch 18/100\n",
      "89/89 - 0s - 3ms/step - loss: 2154420.7500\n",
      "Epoch 19/100\n",
      "89/89 - 0s - 3ms/step - loss: 2145576.5000\n",
      "Epoch 20/100\n",
      "89/89 - 0s - 3ms/step - loss: 2136793.7500\n",
      "Epoch 21/100\n",
      "89/89 - 0s - 2ms/step - loss: 2128007.0000\n",
      "Epoch 22/100\n",
      "89/89 - 0s - 3ms/step - loss: 2119316.2500\n",
      "Epoch 23/100\n",
      "89/89 - 0s - 3ms/step - loss: 2110626.7500\n",
      "Epoch 24/100\n",
      "89/89 - 0s - 3ms/step - loss: 2102019.5000\n",
      "Epoch 25/100\n",
      "89/89 - 0s - 4ms/step - loss: 2093455.1250\n",
      "Epoch 26/100\n",
      "89/89 - 0s - 3ms/step - loss: 2084937.0000\n",
      "Epoch 27/100\n",
      "89/89 - 0s - 3ms/step - loss: 2076434.3750\n",
      "Epoch 28/100\n",
      "89/89 - 0s - 3ms/step - loss: 2067964.1250\n",
      "Epoch 29/100\n",
      "89/89 - 0s - 2ms/step - loss: 2059509.5000\n",
      "Epoch 30/100\n",
      "89/89 - 0s - 2ms/step - loss: 2051128.8750\n",
      "Epoch 31/100\n",
      "89/89 - 0s - 3ms/step - loss: 2042770.5000\n",
      "Epoch 32/100\n",
      "89/89 - 0s - 3ms/step - loss: 2034424.8750\n",
      "Epoch 33/100\n",
      "89/89 - 0s - 3ms/step - loss: 2026054.2500\n",
      "Epoch 34/100\n",
      "89/89 - 0s - 3ms/step - loss: 2017766.6250\n",
      "Epoch 35/100\n",
      "89/89 - 0s - 4ms/step - loss: 2009506.3750\n",
      "Epoch 36/100\n",
      "89/89 - 0s - 3ms/step - loss: 2001301.7500\n",
      "Epoch 37/100\n",
      "89/89 - 0s - 3ms/step - loss: 1993162.1250\n",
      "Epoch 38/100\n",
      "89/89 - 0s - 3ms/step - loss: 1984978.6250\n",
      "Epoch 39/100\n",
      "89/89 - 0s - 4ms/step - loss: 1976865.0000\n",
      "Epoch 40/100\n",
      "89/89 - 0s - 3ms/step - loss: 1968798.6250\n",
      "Epoch 41/100\n",
      "89/89 - 0s - 3ms/step - loss: 1960737.0000\n",
      "Epoch 42/100\n",
      "89/89 - 0s - 3ms/step - loss: 1952706.6250\n",
      "Epoch 43/100\n",
      "89/89 - 0s - 3ms/step - loss: 1944698.7500\n",
      "Epoch 44/100\n",
      "89/89 - 0s - 3ms/step - loss: 1936691.7500\n",
      "Epoch 45/100\n",
      "89/89 - 0s - 3ms/step - loss: 1928737.7500\n",
      "Epoch 46/100\n",
      "89/89 - 0s - 2ms/step - loss: 1920771.2500\n",
      "Epoch 47/100\n",
      "89/89 - 0s - 3ms/step - loss: 1912878.5000\n",
      "Epoch 48/100\n",
      "89/89 - 0s - 2ms/step - loss: 1905027.6250\n",
      "Epoch 49/100\n",
      "89/89 - 0s - 2ms/step - loss: 1897219.2500\n",
      "Epoch 50/100\n",
      "89/89 - 0s - 3ms/step - loss: 1889408.1250\n",
      "Epoch 51/100\n",
      "89/89 - 0s - 4ms/step - loss: 1881572.2500\n",
      "Epoch 52/100\n",
      "89/89 - 0s - 4ms/step - loss: 1873793.2500\n",
      "Epoch 53/100\n",
      "89/89 - 0s - 3ms/step - loss: 1866040.1250\n",
      "Epoch 54/100\n",
      "89/89 - 0s - 3ms/step - loss: 1858326.8750\n",
      "Epoch 55/100\n",
      "89/89 - 0s - 4ms/step - loss: 1850644.0000\n",
      "Epoch 56/100\n",
      "89/89 - 0s - 3ms/step - loss: 1842976.1250\n",
      "Epoch 57/100\n",
      "89/89 - 0s - 4ms/step - loss: 1835321.0000\n",
      "Epoch 58/100\n",
      "89/89 - 0s - 3ms/step - loss: 1827693.1250\n",
      "Epoch 59/100\n",
      "89/89 - 0s - 3ms/step - loss: 1820096.1250\n",
      "Epoch 60/100\n",
      "89/89 - 0s - 3ms/step - loss: 1812540.1250\n",
      "Epoch 61/100\n",
      "89/89 - 0s - 3ms/step - loss: 1804961.7500\n",
      "Epoch 62/100\n",
      "89/89 - 0s - 3ms/step - loss: 1797406.6250\n",
      "Epoch 63/100\n",
      "89/89 - 0s - 3ms/step - loss: 1789864.6250\n",
      "Epoch 64/100\n",
      "89/89 - 0s - 2ms/step - loss: 1782373.2500\n",
      "Epoch 65/100\n",
      "89/89 - 0s - 3ms/step - loss: 1774923.5000\n",
      "Epoch 66/100\n",
      "89/89 - 0s - 2ms/step - loss: 1767499.8750\n",
      "Epoch 67/100\n",
      "89/89 - 0s - 3ms/step - loss: 1760052.8750\n",
      "Epoch 68/100\n",
      "89/89 - 0s - 3ms/step - loss: 1752687.1250\n",
      "Epoch 69/100\n",
      "89/89 - 0s - 3ms/step - loss: 1745341.3750\n",
      "Epoch 70/100\n",
      "89/89 - 0s - 2ms/step - loss: 1738037.7500\n",
      "Epoch 71/100\n",
      "89/89 - 0s - 2ms/step - loss: 1730731.7500\n",
      "Epoch 72/100\n",
      "89/89 - 0s - 3ms/step - loss: 1723463.0000\n",
      "Epoch 73/100\n",
      "89/89 - 0s - 3ms/step - loss: 1716178.7500\n",
      "Epoch 74/100\n",
      "89/89 - 0s - 3ms/step - loss: 1708912.0000\n",
      "Epoch 75/100\n",
      "89/89 - 0s - 3ms/step - loss: 1701701.3750\n",
      "Epoch 76/100\n",
      "89/89 - 0s - 2ms/step - loss: 1694481.3750\n",
      "Epoch 77/100\n",
      "89/89 - 0s - 2ms/step - loss: 1687305.1250\n",
      "Epoch 78/100\n",
      "89/89 - 0s - 3ms/step - loss: 1680142.5000\n",
      "Epoch 79/100\n",
      "89/89 - 0s - 2ms/step - loss: 1673015.2500\n",
      "Epoch 80/100\n",
      "89/89 - 0s - 4ms/step - loss: 1665896.6250\n",
      "Epoch 81/100\n",
      "89/89 - 0s - 3ms/step - loss: 1658822.6250\n",
      "Epoch 82/100\n",
      "89/89 - 0s - 3ms/step - loss: 1651802.7500\n",
      "Epoch 83/100\n",
      "89/89 - 0s - 3ms/step - loss: 1644765.0000\n",
      "Epoch 84/100\n",
      "89/89 - 0s - 3ms/step - loss: 1637785.3750\n",
      "Epoch 85/100\n",
      "89/89 - 0s - 3ms/step - loss: 1630783.0000\n",
      "Epoch 86/100\n",
      "89/89 - 0s - 3ms/step - loss: 1623814.0000\n",
      "Epoch 87/100\n",
      "89/89 - 0s - 3ms/step - loss: 1616878.8750\n",
      "Epoch 88/100\n",
      "89/89 - 0s - 3ms/step - loss: 1609921.0000\n",
      "Epoch 89/100\n",
      "89/89 - 0s - 3ms/step - loss: 1603017.3750\n",
      "Epoch 90/100\n",
      "89/89 - 0s - 3ms/step - loss: 1596155.1250\n",
      "Epoch 91/100\n",
      "89/89 - 0s - 3ms/step - loss: 1589306.8750\n",
      "Epoch 92/100\n",
      "89/89 - 0s - 3ms/step - loss: 1582459.2500\n",
      "Epoch 93/100\n",
      "89/89 - 0s - 2ms/step - loss: 1575648.2500\n",
      "Epoch 94/100\n",
      "89/89 - 0s - 4ms/step - loss: 1568845.7500\n",
      "Epoch 95/100\n",
      "89/89 - 0s - 3ms/step - loss: 1562092.6250\n",
      "Epoch 96/100\n",
      "89/89 - 0s - 3ms/step - loss: 1555365.0000\n",
      "Epoch 97/100\n",
      "89/89 - 0s - 3ms/step - loss: 1548688.0000\n",
      "Epoch 98/100\n",
      "89/89 - 0s - 3ms/step - loss: 1541984.6250\n",
      "Epoch 99/100\n",
      "89/89 - 0s - 3ms/step - loss: 1535316.7500\n",
      "Epoch 100/100\n",
      "89/89 - 0s - 3ms/step - loss: 1528695.2500\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step \n",
      "LSTM Model - Annualized Returns: 0.0, Sharpe Ratio: nan, Max Drawdown: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_14904\\2221612596.py:212: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  sharpe_ratio = annualized_returns / annualized_volatility\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load CSV data\n",
    "df = pd.read_csv(\"tcs_data.csv\")\n",
    "\n",
    "# Define the trading environment\n",
    "class TradingEnv:\n",
    "    def __init__(self, df, initial_capital=10000):\n",
    "        self.df = df\n",
    "        self.current_step = 0\n",
    "        self.action_space = [0, 1, 2]  # 0: Hold, 1: Buy, 2: Sell\n",
    "        self.current_position = 0\n",
    "        self.capital = initial_capital\n",
    "        self.last_buy_price = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.current_position = 0\n",
    "        self.capital = 10000\n",
    "        self.last_buy_price = 0\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        row = self.df.iloc[self.current_step]\n",
    "        state = [\n",
    "            row['EMA_12'],\n",
    "            row['MACD'],\n",
    "            row['VWAP'],\n",
    "            row['RSI'],\n",
    "            row['ATR'],\n",
    "            row['Supertrend'],\n",
    "            row['Upper_Band'],\n",
    "            row['Lower_Band'],\n",
    "            row['Fibonacci_R1'],\n",
    "            row['Fibonacci_S1'],\n",
    "            row['Momentum']\n",
    "        ]\n",
    "        return np.array(state)\n",
    "\n",
    "    def step(self, action):\n",
    "        row = self.df.iloc[self.current_step]\n",
    "        stock_price = row['Close']\n",
    "        reward = 0\n",
    "\n",
    "        if action == 1:  # Buy\n",
    "            if self.capital >= stock_price:\n",
    "                shares_to_buy = self.capital // stock_price\n",
    "                self.capital -= shares_to_buy * stock_price\n",
    "                self.current_position += shares_to_buy\n",
    "                self.last_buy_price = stock_price\n",
    "            reward = 0\n",
    "\n",
    "        elif action == 2:  # Sell\n",
    "            if self.current_position > 0:\n",
    "                shares_to_sell = self.current_position\n",
    "                self.capital += shares_to_sell * stock_price\n",
    "                profit = (stock_price - self.last_buy_price) * shares_to_sell\n",
    "                reward = profit\n",
    "                self.current_position = 0\n",
    "\n",
    "        elif action == 0:  # Hold\n",
    "            if self.current_position > 0:\n",
    "                reward = (stock_price - self.last_buy_price) * self.current_position\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.df) - 1\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "# Define the DQN model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Train the DQN agent\n",
    "def train_agent(env, model, target_model, optimizer, criterion, episodes=100, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "    memory = deque(maxlen=2000)\n",
    "    batch_size = 128\n",
    "    target_update = 10\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice(env.action_space)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = model(state)\n",
    "                    action = torch.argmax(q_values).item()\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "            total_reward += reward\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if len(memory) > batch_size:\n",
    "                batch = random.sample(memory, batch_size)\n",
    "                train_step(batch, model, target_model, optimizer, criterion, gamma)\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "        # Epsilon decay\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "        # Update the target model\n",
    "        if episode % target_update == 0:\n",
    "            target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}\")\n",
    "\n",
    "    return rewards\n",
    "\n",
    "def train_step(batch, model, target_model, optimizer, criterion, gamma):\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    states = torch.stack(states).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    next_states = torch.stack(next_states).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    q_values = model(states).gather(1, actions)\n",
    "    next_q_values = target_model(next_states).max(1)[0].unsqueeze(1)\n",
    "    target_q_values = rewards + (gamma * next_q_values * (1 - dones))\n",
    "\n",
    "    loss = criterion(q_values, target_q_values)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate the trained agent\n",
    "def evaluate_agent(env, model):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state)\n",
    "            action = torch.argmax(q_values).item()\n",
    "\n",
    "        next_state, reward, done = env.step(action)\n",
    "        state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
    "        total_reward += reward\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "# Define and train ARIMA model\n",
    "def arima_forecast(train_data, test_data):\n",
    "    model = sm.tsa.ARIMA(train_data, order=(5, 1, 0))  # Adjust parameters as needed\n",
    "    model_fit = model.fit()\n",
    "    forecast = model_fit.forecast(steps=len(test_data))\n",
    "    return forecast\n",
    "\n",
    "# Define and train LSTM model\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "def lstm_forecast(train_data, test_data):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(train_data.reshape(-1, 1))\n",
    "    \n",
    "    model = create_lstm_model((scaled_data.shape[1], 1))\n",
    "    model.fit(scaled_data, train_data, epochs=100, batch_size=32, verbose=2)\n",
    "    \n",
    "    scaled_test_data = scaler.transform(test_data.reshape(-1, 1))\n",
    "    forecast = model.predict(scaled_test_data)\n",
    "    return forecast\n",
    "\n",
    "# Performance metrics\n",
    "def calculate_performance_metrics(returns):\n",
    "    cumulative_returns = np.cumprod(1 + returns) - 1\n",
    "    annualized_returns = np.mean(returns) * 252  # Assuming daily returns\n",
    "    annualized_volatility = np.std(returns) * np.sqrt(252)\n",
    "    sharpe_ratio = annualized_returns / annualized_volatility\n",
    "    max_drawdown = np.min(cumulative_returns)  # Simplified calculation\n",
    "    return annualized_returns, sharpe_ratio, max_drawdown\n",
    "\n",
    "# Data preprocessing and train/test split\n",
    "def preprocess_data(df):\n",
    "    features = df[['EMA_12', 'MACD', 'VWAP', 'RSI', 'ATR', 'Supertrend', 'Upper_Band', 'Lower_Band', 'Fibonacci_R1', 'Fibonacci_S1', 'Momentum']].values\n",
    "    prices = df['Close'].values\n",
    "    return features, prices\n",
    "\n",
    "def split_data(features, prices, split_ratio=0.8):\n",
    "    split_idx = int(len(features) * split_ratio)\n",
    "    train_features, test_features = features[:split_idx], features[split_idx:]\n",
    "    train_prices, test_prices = prices[:split_idx], prices[split_idx:]\n",
    "    return train_features, test_features, train_prices, test_prices\n",
    "\n",
    "# Main execution\n",
    "features, prices = preprocess_data(df)\n",
    "train_features, test_features, train_prices, test_prices = split_data(features, prices)\n",
    "\n",
    "# Train DQN agent\n",
    "state_size = len(features[0])\n",
    "action_size = 3\n",
    "env = TradingEnv(df)\n",
    "\n",
    "model = DQN(state_size, action_size).to(device)\n",
    "target_model = DQN(state_size, action_size).to(device)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "rewards = train_agent(env, model, target_model, optimizer, criterion)\n",
    "\n",
    "# Evaluate the trained RL agent\n",
    "total_reward = evaluate_agent(env, model)\n",
    "print(f\"Total Reward of RL Agent: {total_reward}\")\n",
    "\n",
    "# Forecasting with ARIMA\n",
    "arima_forecast_result = arima_forecast(train_prices, test_prices)\n",
    "arima_metrics = calculate_performance_metrics(np.diff(arima_forecast_result))\n",
    "print(f\"ARIMA Model - Annualized Returns: {arima_metrics[0]}, Sharpe Ratio: {arima_metrics[1]}, Max Drawdown: {arima_metrics[2]}\")\n",
    "\n",
    "# Forecasting with LSTM\n",
    "lstm_forecast_result = lstm_forecast(train_prices, test_prices)\n",
    "lstm_metrics = calculate_performance_metrics(np.diff(lstm_forecast_result.flatten()))\n",
    "print(f\"LSTM Model - Annualized Returns: {lstm_metrics[0]}, Sharpe Ratio: {lstm_metrics[1]}, Max Drawdown: {lstm_metrics[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d466b023-306a-4a0b-b529-701df13aeef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736d36ab-53fd-4d0c-b955-329711aec6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
